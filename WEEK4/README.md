# Lab 4 â€“ Model Selection and Comparative Analysis  

**Name:** SHARATH GOWDA GR  
**SRN:** PES2UG24CS823  
**Course:** Machine Learning  
**Lab:** WEEK 4  

---
as mentioned in class only code file pushed
---

## ğŸ“– Introduction  
In this lab, I explored **model selection and hyperparameter tuning** for different classifiers. The experiment was divided into two key parts:  

1. ğŸ”„ **Manual Grid Search** â€“ Implementing grid search using loops and cross-validation.  
2. âš¡ **Automated Grid Search (GridSearchCV)** â€“ Using scikit-learnâ€™s built-in method.  

The main goal was to understand **how grid search works internally** and then compare it with the **efficient automated version**.  

---

## ğŸ“Š Datasets Used  
The datasets were preprocessed (scaling, feature selection, encoding if needed) and split into training/testing sets:  

- ğŸ· **Wine Quality** â†’ Predicting good/bad quality wine.  
- ğŸ§‘â€ğŸ’¼ **HR Attrition** â†’ Predicting employee attrition.  
- ğŸ’µ **Banknote Authentication** â†’ Predicting genuine vs forged banknotes.  
- âš—ï¸ **QSAR Biodegradation** â†’ Predicting chemical biodegradability.  

---

## âš™ï¸ Methodology  

ğŸ”— **Pipeline Used:**  

- ğŸ“ **StandardScaler** â†’ Normalized the features.  
- âœ‚ï¸ **SelectKBest** â†’ Selected best features (parameter `k` tuned).  
- ğŸ¤– **Classifiers applied:**  
  - ğŸŒ³ Decision Tree  
  - ğŸ‘¥ k-Nearest Neighbors (kNN)  
  - â— Logistic Regression  

---

## ğŸ› ï¸ Part 1: Manual Grid Search  

- Defined parameter grids for each classifier:  
  - Decision Tree â†’ `max_depth`  
  - kNN â†’ `n_neighbors`  
  - Logistic Regression â†’ `C`  

- Performed **5-fold Stratified Cross-Validation**.  
- Trained pipeline â†’ predicted probabilities â†’ calculated **ROC-AUC**.  
- Stored mean scores â†’ selected **best parameters**.  
- Retrained best pipeline on full training set.  

---

## âš¡ Part 2: GridSearchCV  

- Built the same pipeline.  
- Used **GridSearchCV** with:  
  - Parameter grid  
  - `cv=5` (Stratified)  
  - `scoring="roc_auc"`  

- Reported **best parameters, best score, and best estimator**.  

---

## ğŸ“ˆ Results & Observations  

- âœ… Both **manual grid search** and **GridSearchCV** gave **consistent results**.  
- âš¡ GridSearchCV was **faster and more convenient**.  
- ğŸ§  Manual grid search improved my **understanding of hyperparameter tuning**.  
- ğŸ“Š Plotted **Confusion Matrices** and **ROC Curves** for final models.  

*(Add screenshots/plots here if available)*  

---

## ğŸ Conclusion  

Key learnings from this lab:  

âœ”ï¸ Use **pipelines** to prevent data leakage.  
âœ”ï¸ Apply **hyperparameter tuning** to improve performance.  
âœ”ï¸ Use **cross-validation** for robust evaluation.  
âœ”ï¸ **Manual methods** help learning, but **GridSearchCV** is best for real-world projects.  

---

## ğŸš€ Tech Stack  

- ğŸ Python  
- ğŸ“¦ scikit-learn  
- ğŸ“Š matplotlib / seaborn (for visualization)  
- ğŸ”¢ numpy / pandas  


---

## ğŸ“Œ Author  
**ğŸ‘¤ SHARATH GOWDA GR**  
SRN: PES2UG24CS823  
