# INCLUDED ALL THE WEEKS SHORT EXPLAINATION DOWN
# Week-03 â€“ Decision Tree (ID3)

PES2UG24CS823

This folder contains all the files for **Week 3 Lab: ID3 Decision Tree Algorithm and Its Explaination**.

Everything is already organized and structured for easy navigation.  
Go through the files in this folder to understand the implementation and datasets.

---

## Folder Structure
- **EC_F_823_Lab3.py** â€“ My implementation of the ID3 helper functions
- **student_lab.py** â€“ Original lab skeleton (for reference)
- **test.py** â€“ Testing and evaluation script (provided)
- **mushroom.csv** â€“ Dataset 1
- **tictactoe.csv** â€“ Dataset 2
- **Nursery.csv** â€“ Dataset 3
- **README.md** â€“ This file
- **temp.txt** â€“ Temporary/extra notes (if any)
- **.gitignore** â€“ Git ignore rules

---

## How to Use
Just open this folder and run the **test.py** script with the required dataset.

Example:
```bash
python test.py --ID EC_F_823_Lab3 --data mushroom.csv
```

---

# WEEK-04 â€“ Model Selection & Comparative Analysis   

---

## Highlights  
- Compared Manual Grid Search vs GridSearchCV  
- Datasets: Wine Quality, HR Attrition, Banknote Authentication, QSAR Biodegradation  
- Pipeline: StandardScaler â†’ SelectKBest â†’ Classifier  
- Models: Decision Tree, k-Nearest Neighbors, Logistic Regression  
- Both methods gave consistent results; GridSearchCV was faster  
- Generated confusion matrices and ROC curves  
- Key learnings: pipelines, hyperparameter tuning, cross-validation  

---

## Tools  
Python, scikit-learn, pandas, numpy, matplotlib, seaborn  
---

## Full explained inside the folder
## Feel Free To Fork this Repository

---
# WEEK-06 Neural Network Function Approximation 
---

## ðŸ“Œ Overview
Implemented a **neural network from scratch** to approximate a polynomial dataset generated from my SRN. Learned how activation functions, initialization, and hyperparameters affect training.

---
## ðŸ”‘ Key Learnings
- **ReLU > Sigmoid** â†’ ReLU gave much better results.  
- **Xavier initialization** prevented vanishing/exploding gradients.  
- **Learning rate (0.005)** and **batch size (32)** worked best.  
- Best model achieved **RÂ² â‰ˆ 0.86** on test data.  
- Low training loss â‰  good generalization â†’ balance is key.  

---

## ðŸ“Š Best Result
- **Config:** ReLU, LR=0.005, Batch=32, Epochs=500  
- **Final Test Loss:** ~0.137  
- **RÂ² Score:** ~0.867  

---
